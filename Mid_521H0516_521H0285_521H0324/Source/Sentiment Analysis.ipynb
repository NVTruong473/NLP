{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "24bfb177-0798-462e-8c8d-ff69fb643a49",
      "metadata": {
        "id": "24bfb177-0798-462e-8c8d-ff69fb643a49"
      },
      "source": [
        "# Classification for Vietnamese Text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: UIT-VSMEC (http://nlp.uit.edu.vn/datasets/)"
      ],
      "metadata": {
        "id": "UVv79wLs5g_6"
      },
      "id": "UVv79wLs5g_6"
    },
    {
      "cell_type": "markdown",
      "id": "5c21df55-fde5-4913-ac94-b48490de2feb",
      "metadata": {
        "id": "5c21df55-fde5-4913-ac94-b48490de2feb"
      },
      "source": [
        "## Set up"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages\n",
        "!pip install pandas underthesea emoji demoji langdetect gensim huggingface_hub"
      ],
      "metadata": {
        "id": "Vo3qTxVqxXlq",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a44e8ee-5903-4fca-ccc8-048ead3968c8"
      },
      "id": "Vo3qTxVqxXlq",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: underthesea in /usr/local/lib/python3.11/dist-packages (6.8.4)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.14.1)\n",
            "Requirement already satisfied: demoji in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.11/dist-packages (from underthesea) (8.1.8)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.11/dist-packages (from underthesea) (0.9.11)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from underthesea) (3.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from underthesea) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from underthesea) (2.32.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.4.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.6.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from underthesea) (6.0.2)\n",
            "Requirement already satisfied: underthesea-core==1.0.4 in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Collecting numpy>=1.23.2 (from pandas)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->underthesea) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (3.6.0)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.4\n",
            "    Uninstalling numpy-2.2.4:\n",
            "      Successfully uninstalled numpy-2.2.4\n",
            "Successfully installed numpy-1.26.4\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install --upgrade --force-reinstall numpy\n",
        "!pip install --upgrade --force-reinstall pandas"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4-e8s68F50gm",
        "outputId": "e36c5552-e4fc-400b-98ef-9b801f787472"
      },
      "id": "4-e8s68F50gm",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n",
            "  Using cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Using cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.4\n",
            "Collecting pandas\n",
            "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Collecting numpy>=1.23.2 (from pandas)\n",
            "  Using cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m507.9/507.9 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.1\n",
            "    Uninstalling pytz-2025.1:\n",
            "      Successfully uninstalled pytz-2025.1\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.1\n",
            "    Uninstalling tzdata-2025.1:\n",
            "      Successfully uninstalled tzdata-2025.1\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.4\n",
            "    Uninstalling numpy-2.2.4:\n",
            "      Successfully uninstalled numpy-2.2.4\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.4 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.4 pandas-2.2.3 python-dateutil-2.9.0.post0 pytz-2025.1 six-1.17.0 tzdata-2025.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "six"
                ]
              },
              "id": "bbe19649183a4beeb5cc37ec1a612f40"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bd9cc9fe-eafb-431d-b0d8-69de7dec7382",
      "metadata": {
        "id": "bd9cc9fe-eafb-431d-b0d8-69de7dec7382",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "ce91b027-2834-45e8-833a-517889b4f8f0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-42bce034ad67>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import libraries for general purpose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2149\u001b[0m )\n\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2151\u001b[0;31m from torch import (\n\u001b[0m\u001b[1;32m   2152\u001b[0m     \u001b[0m__config__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m__config__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2153\u001b[0m     \u001b[0m__future__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m__future__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/profiler/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_optimizer_step_post_hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m from .profiler import (\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0m_KinetoProfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mExecutionTraceObserver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/profiler/profiler.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkineto_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProfilerActivity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_memory_profiler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMemoryProfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMemoryProfileTimeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/profiler/_memory_profiler.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    200\u001b[0m def extract_gradients(\n\u001b[1;32m    201\u001b[0m     \u001b[0mnode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_ProfilerEvent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m ) -> Iterator[Tuple[Optional[TensorKey], TensorKey]]:\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_extract_parameters_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mp_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/typing.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mpass\u001b[0m  \u001b[0;31m# All real errors (not unhashable args) are raised below.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/typing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m   1591\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_type_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1592\u001b[0m         \u001b[0m_check_generic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcopy_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/typing.py\u001b[0m in \u001b[0;36mcopy_with\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m   1594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcopy_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1596\u001b[0;31m         return _GenericAlias(self.__origin__, params,\n\u001b[0m\u001b[1;32m   1597\u001b[0m                              name=self._name, inst=self._inst)\n\u001b[1;32m   1598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/typing.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, origin, args, inst, name, _paramspec_tvars)\u001b[0m\n\u001b[1;32m   1381\u001b[0m         self.__args__ = tuple(... if a is _TypingEllipsis else\n\u001b[1;32m   1382\u001b[0m                               a for a in args)\n\u001b[0;32m-> 1383\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__parameters__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collect_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_paramspec_tvars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_paramspec_tvars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/typing_extensions.py\u001b[0m in \u001b[0;36m_collect_parameters\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   3077\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_is_unpacked_typevartuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m                     \u001b[0mtype_var_tuple_encountered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3079\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__parameters__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3080\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m                         \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Import libraries for general purpose\n",
        "import json\n",
        "import torch\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoTokenizer, RobertaForSequenceClassification, BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Text cleaning\n",
        "import re\n",
        "import emoji\n",
        "import demoji\n",
        "import unicodedata\n",
        "from langdetect import detect, LangDetectException\n",
        "from underthesea import word_tokenize, text_normalize\n",
        "\n",
        "# Data preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Gensim for Doc2Vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "# sklearn imports\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix, make_scorer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn import linear_model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Set style for plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.despine()\n",
        "# plt.style.use(\"seaborn-whitegrid\")\n",
        "plt.rc(\"figure\", autolayout=True)\n",
        "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)\n",
        "\n",
        "# Handle warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "8d8BB7rOCktl"
      },
      "id": "8d8BB7rOCktl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()"
      ],
      "metadata": {
        "id": "Vpg-20e3EnkN"
      },
      "id": "Vpg-20e3EnkN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Build a dataset and preprocess\n"
      ],
      "metadata": {
        "id": "46v4Gkd01lQ7"
      },
      "id": "46v4Gkd01lQ7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load dataset\n"
      ],
      "metadata": {
        "id": "nSAZtErj742J"
      },
      "id": "nSAZtErj742J"
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "  df = pd.read_excel(path, sheet_name='Sheet1')\n",
        "  df.columns = ['index', 'Emotion', 'Sentence']\n",
        "  # remove unused column\n",
        "  df.drop(columns=['index'], inplace=True)\n",
        "  return df\n",
        "\n",
        "train_df = load_data('https://github.com/patuaans/vietnamese-sentiment-analysis/blob/main/train_nor_811.xlsx?raw=true')\n",
        "val_df = load_data('https://github.com/patuaans/vietnamese-sentiment-analysis/blob/main/valid_nor_811.xlsx?raw=true')\n",
        "test_df = load_data('https://github.com/patuaans/vietnamese-sentiment-analysis/blob/main/test_nor_811.xlsx?raw=true')\n",
        "\n",
        "# Concatenate all DataFrames into one\n",
        "df = pd.concat([train_df, val_df, test_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "9GY0ijhC-P1-"
      },
      "id": "9GY0ijhC-P1-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "QaL3hQDZEX0N"
      },
      "id": "QaL3hQDZEX0N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "f38SsR3BE4gV"
      },
      "id": "f38SsR3BE4gV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**=> No Missing Values**"
      ],
      "metadata": {
        "id": "RCQ9CPHlFP4-"
      },
      "id": "RCQ9CPHlFP4-"
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "XFl8iCWieaOE"
      },
      "id": "XFl8iCWieaOE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**=> There are some duplicated comments, we will remove them**"
      ],
      "metadata": {
        "id": "VK3ZcOZAef1n"
      },
      "id": "VK3ZcOZAef1n"
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[~df.duplicated()]"
      ],
      "metadata": {
        "id": "tYcaG7yf9xoS"
      },
      "id": "tYcaG7yf9xoS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "ST0wBwmZ-BFd"
      },
      "id": "ST0wBwmZ-BFd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_sentiments = df.Emotion.unique()\n",
        "num_labels = len(unique_sentiments)\n",
        "unique_sentiments"
      ],
      "metadata": {
        "id": "QkrySOAnHZGP"
      },
      "id": "QkrySOAnHZGP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**=> 7 labels**"
      ],
      "metadata": {
        "id": "kdE-GO8JenN2"
      },
      "id": "kdE-GO8JenN2"
    },
    {
      "cell_type": "code",
      "source": [
        "ax = df.Emotion.value_counts().plot.bar()\n",
        "ax.bar_label(ax.containers[0])\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
        "ax.set_yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RniDC0EiFXzE"
      },
      "id": "RniDC0EiFXzE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**=> Imbalanced labels! But We still want to classify 7 classes**"
      ],
      "metadata": {
        "id": "FVx8E-tPGNxZ"
      },
      "id": "FVx8E-tPGNxZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# mapping to the sentiment column\n",
        "dicto = {'Disgust': 0, 'Enjoyment': 1 , 'Anger': 2, 'Surprise': 3, 'Sadness': 4, 'Fear': 5, 'Other': 6}\n",
        "\n",
        "df.Emotion = df.Emotion.map(dicto)"
      ],
      "metadata": {
        "id": "HiU_hS4UH3Vs"
      },
      "id": "HiU_hS4UH3Vs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "JG5apdm0IDVp"
      },
      "id": "JG5apdm0IDVp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocessing"
      ],
      "metadata": {
        "id": "5QxR2OsFIUep"
      },
      "id": "5QxR2OsFIUep"
    },
    {
      "cell_type": "markdown",
      "id": "24064bb2-fbde-4d07-8bd6-848781f13de6",
      "metadata": {
        "id": "24064bb2-fbde-4d07-8bd6-848781f13de6"
      },
      "source": [
        "#### Load teencode dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce339c3b-6650-4bcc-aa4a-d86b55defbb6",
      "metadata": {
        "id": "ce339c3b-6650-4bcc-aa4a-d86b55defbb6"
      },
      "outputs": [],
      "source": [
        "def load_teencode_dict(file_path):\n",
        "    teencode_dict = {}\n",
        "    f = requests.get(file_path)\n",
        "    teencode_dict = f.json()\n",
        "    return teencode_dict\n",
        "\n",
        "# Load the dictionary\n",
        "teencode_dict = load_teencode_dict('https://github.com/patuaans/vietnamese-sentiment-analysis/blob/main/teencode.json?raw=true')\n",
        "print(teencode_dict)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_teencode(text, teencode_dict):\n",
        "    tokens = word_tokenize(text)\n",
        "    normalized_words = [teencode_dict.get(word, word) for word in tokens]\n",
        "    return ' '.join(normalized_words)\n",
        "\n",
        "replace_teencode('ctrai kh√¥g bme', teencode_dict)"
      ],
      "metadata": {
        "id": "SDhOcOplejcN"
      },
      "id": "SDhOcOplejcN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convert emoticons to emoji"
      ],
      "metadata": {
        "id": "2fhcHP_E5Uhl"
      },
      "id": "2fhcHP_E5Uhl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary mapping text emoticons to emoji with proper escaping\n",
        "emoticon_to_emoji = {\n",
        "    r'\\?{1,}': '?',\n",
        "    r'\\!{1,}': '!',\n",
        "    r'=\\]{1,}': 'üòä',     # =] or =]] or =]]] etc.\n",
        "    r':\\){1,}': 'üòä',     # :) or :)) or :))) etc.\n",
        "    r'=D': 'üòÉ',\n",
        "    r'=d': 'üòÉ',\n",
        "    r'< 3': '‚ù§Ô∏è',\n",
        "    r'=\\){1,}': 'üòÇ',     # =) or =)) or =))) etc.\n",
        "    r':\\({1,}': 'üò¢',     # :( or :(( or :((( etc.\n",
        "    r':-\\({1,}': 'üò¢',    # :-( or :-(( etc.\n",
        "    r';\\){1,}': 'üòâ',     # ;) or ;)) etc.\n",
        "    r':D': 'üòÑ',\n",
        "    r':d': 'üòÑ',\n",
        "    r':P': 'üòú',\n",
        "    r':p': 'üòú',\n",
        "    r'T_T': 'üò≠',\n",
        "    r't_t': 'üò≠',\n",
        "    r'-_-': 'üòë',\n",
        "    r'@@': 'üò£',\n",
        "    r'XD': 'üòÇ',\n",
        "    r'xd': 'üòÇ',\n",
        "    r':v': 'üòö',\n",
        "    r':3': 'üò∫',\n",
        "    r'\\^-\\^': 'üòä',\n",
        "}\n",
        "\n",
        "def convert_emoticons_to_emoji(text):\n",
        "    for emoticon_pattern, emoji_char in emoticon_to_emoji.items():\n",
        "        text = re.sub(emoticon_pattern, emoji_char, text)\n",
        "    return text\n",
        "\n",
        "# Test the function\n",
        "test_sentences = [\n",
        "    \"C·∫£m ∆°n b·∫°n nh√© :))))\",\n",
        "    \"nay bu·ªìn :((((\",\n",
        "    \"Vui qu√° =))))\",\n",
        "    \"Th√≠ch qu√° :D\",\n",
        "    \"Huhu bu·ªìn l·∫Øm T_T\",\n",
        "    \"C∆∞·ªùi x·ªâu lu√¥n =D\",\n",
        "    \"Hihi d·ªÖ th∆∞∆°ng qu√° ^-^\",\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    converted = convert_emoticons_to_emoji(sentence)\n",
        "    print(f\"Original: {sentence}\")\n",
        "    print(f\"Converted: {converted}\\n\")"
      ],
      "metadata": {
        "id": "jsNXFaKG5cG6"
      },
      "id": "jsNXFaKG5cG6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2495d976-7aa1-48a8-8bbc-df0ba73bc2f1",
      "metadata": {
        "id": "2495d976-7aa1-48a8-8bbc-df0ba73bc2f1"
      },
      "source": [
        "#### Preprocessing Function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_hashtags(text):\n",
        "    # Remove hashtags at the end of the sentence\n",
        "    text = re.sub(r'(\\s+#[\\w-]+)+\\s*$', '', text).strip()\n",
        "    # Remove the # symbol from hashtags in the middle of the sentence\n",
        "    text = re.sub(r'#([\\w-]+)', r'\\1', text).strip()\n",
        "    return text\n",
        "\n",
        "clean_hashtags('cccc #aaaa ddd #bbbb')"
      ],
      "metadata": {
        "id": "i6TxXMIrbfGY"
      },
      "id": "i6TxXMIrbfGY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def filter_non_vietnamese(text):\n",
        "#   try:\n",
        "#     lang = detect(text)\n",
        "#   except LangDetectException:\n",
        "#     lang = 'unknown'\n",
        "#   return text if lang == 'vi' else ''\n",
        "\n",
        "# filter_non_vietnamese('run c√≥ ‚ò∫Ô∏è')"
      ],
      "metadata": {
        "id": "Qln0mL_fcjsO"
      },
      "id": "Qln0mL_fcjsO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_short_words(text, min_len=2):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if len(word) >= min_len]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "remove_short_words('c')"
      ],
      "metadata": {
        "id": "OtB7NEpqdcVM"
      },
      "id": "OtB7NEpqdcVM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1fbed38-bb8e-44ad-abe5-043ebbba91fd",
      "metadata": {
        "id": "e1fbed38-bb8e-44ad-abe5-043ebbba91fd"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    # Change to Unicode d·ª±ng s·∫µn\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+', '', text)\n",
        "    # Remove mentions\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    # Remove hashtag\n",
        "    text = clean_hashtags(text)\n",
        "    # Convert emoticons to emojis\n",
        "    text = convert_emoticons_to_emoji(text)\n",
        "    # Convert emojis to text descriptions\n",
        "    text = emoji.demojize(text)\n",
        "    # Clean emojis\n",
        "    text = demoji.replace(text, '')\n",
        "    # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát (kh√¥ng lo·∫°i b·ªè ch·ªØ c√°i ti·∫øng Vi·ªát v√† kho·∫£ng tr·∫Øng)\n",
        "    text = re.sub(r\"[^0-9a-zA-Z√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªô·ªó∆°·ªõ·ªù·ªü·ª£·ª°√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ_\\s]\", \" \", text)\n",
        "    # Replace teencode with standard words\n",
        "    text= replace_teencode(text, teencode_dict)\n",
        "    # Collapse repeated emoji descriptions\n",
        "    text = re.sub(r\"\\b(\\w+)( \\1\\b)+\", r\"\\1\", text)\n",
        "    # Collapse repeated characters\n",
        "    text = re.sub(r'(\\w)(\\1{1,})', r'\\1', text)\n",
        "    # Text normalize\n",
        "    normalized_words = text_normalize(text)\n",
        "    return normalized_words\n",
        "\n",
        "comment = '   ctrai Check    this out! https://example.com @user123 #amazing C·∫£m ∆°n b·∫°n! ‚ò∫Ô∏è‚ò∫Ô∏è   '\n",
        "preprocess_text(comment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0139bf56-0bac-494d-81b8-ed4017d162e7",
      "metadata": {
        "id": "0139bf56-0bac-494d-81b8-ed4017d162e7"
      },
      "outputs": [],
      "source": [
        "# Apply preprocessing to the 'Sentence' column\n",
        "df['text_clean'] = df['Sentence'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "pxMqBX35iHQs"
      },
      "id": "pxMqBX35iHQs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text_clean'].duplicated().sum()"
      ],
      "metadata": {
        "id": "L-Uw8Xd3iRDi"
      },
      "id": "L-Uw8Xd3iRDi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates('text_clean', inplace=True)"
      ],
      "metadata": {
        "id": "x146arkeATR4"
      },
      "id": "x146arkeATR4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "RUnrbktkjYQi"
      },
      "id": "RUnrbktkjYQi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "mGa6cEqMP9Lt"
      },
      "id": "mGa6cEqMP9Lt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comments length analysis"
      ],
      "metadata": {
        "id": "o7KfTgfrkmoJ"
      },
      "id": "o7KfTgfrkmoJ"
    },
    {
      "cell_type": "code",
      "source": [
        "df['text_len'] = [len(text.split()) for text in df.text_clean]"
      ],
      "metadata": {
        "id": "ULrulS6llAaa"
      },
      "id": "ULrulS6llAaa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "ax = sns.countplot(x='text_len', data=df[df['text_len']<10], palette='mako')\n",
        "plt.title('Count of comments with less than 10 words', fontsize=20)\n",
        "plt.yticks([])\n",
        "for container in ax.containers:\n",
        "    ax.bar_label(container)\n",
        "plt.ylabel('count')\n",
        "plt.xlabel('')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FSJCHbV-kqMD"
      },
      "id": "FSJCHbV-kqMD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort_values(by=['text_len'], ascending=False)"
      ],
      "metadata": {
        "id": "3lplvuoJpehI"
      },
      "id": "3lplvuoJpehI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Classical model with diffrent method"
      ],
      "metadata": {
        "id": "l3CcABP814Mo"
      },
      "id": "l3CcABP814Mo"
    },
    {
      "cell_type": "code",
      "source": [
        "train_valid_df, test_df = train_test_split(df[['text_clean', 'Emotion']], test_size=0.2, random_state=42)\n",
        "train_df, valid_df = train_test_split(train_valid_df, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "brMRvfLrQ1Wj"
      },
      "id": "brMRvfLrQ1Wj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classical Model"
      ],
      "metadata": {
        "id": "5ubOZFyp6Z0B"
      },
      "id": "5ubOZFyp6Z0B"
    },
    {
      "cell_type": "code",
      "source": [
        "def classical_model(train_df, test_df , bow=False, TFIDF=False, Ngram=False,\n",
        "                    model=linear_model.LogisticRegression(solver='liblinear')):\n",
        "    if bow:\n",
        "        count_vec = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
        "    elif TFIDF:\n",
        "        count_vec = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
        "    elif Ngram:\n",
        "        count_vec = CountVectorizer(tokenizer=word_tokenize, token_pattern=None, ngram_range=Ngram)\n",
        "\n",
        "    count_vec.fit(train_df.text_clean)\n",
        "    xtrain = count_vec.transform(train_df.text_clean)\n",
        "    xtest = count_vec.transform(test_df.text_clean)\n",
        "    model.fit(xtrain, train_df.Emotion)\n",
        "    preds = model.predict(xtest)\n",
        "    accuracy_precision = precision_score(test_df.Emotion, preds, average='macro')\n",
        "    accuracy_recall = recall_score(test_df.Emotion, preds, average='macro')\n",
        "    print('precision score:', accuracy_precision)\n",
        "    print('recall score:', accuracy_recall)\n",
        "    print(\"========================================================\")\n",
        "\n",
        "    print(classification_report(test_df.Emotion, preds))"
      ],
      "metadata": {
        "id": "2qkHaAnr31lQ"
      },
      "id": "2qkHaAnr31lQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Logistic"
      ],
      "metadata": {
        "id": "TqNgqrjb-FPc"
      },
      "id": "TqNgqrjb-FPc"
    },
    {
      "cell_type": "code",
      "source": [
        "# BoW\n",
        "classical_model(train_df, test_df , bow =True,model=linear_model.LogisticRegression(solver = 'liblinear'))"
      ],
      "metadata": {
        "id": "WWkqS_FI6Haa"
      },
      "id": "WWkqS_FI6Haa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "classical_model(train_df, test_df , TFIDF=True,model=linear_model.LogisticRegression(solver = 'liblinear'))"
      ],
      "metadata": {
        "id": "C6wwVNS9-JJO"
      },
      "id": "C6wwVNS9-JJO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N-gram\n",
        "classical_model(train_df, test_df , Ngram=(1,2),model=linear_model.LogisticRegression(solver = 'liblinear'))"
      ],
      "metadata": {
        "id": "5T1Glyvq-Nl4"
      },
      "id": "5T1Glyvq-Nl4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naive"
      ],
      "metadata": {
        "id": "7lQsfBOh-cqF"
      },
      "id": "7lQsfBOh-cqF"
    },
    {
      "cell_type": "code",
      "source": [
        "# BoW\n",
        "classical_model(train_df, test_df ,bow = True,model = MultinomialNB()) # multiclassification"
      ],
      "metadata": {
        "id": "FE77ydGH6jOk"
      },
      "id": "FE77ydGH6jOk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "classical_model(train_df, test_df, model= MultinomialNB(),TFIDF=True)"
      ],
      "metadata": {
        "id": "Sf7OY6N07cZd"
      },
      "id": "Sf7OY6N07cZd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N-gram\n",
        "classical_model(train_df, test_df, model = MultinomialNB(), Ngram=(1,2))"
      ],
      "metadata": {
        "id": "S1WiMX3O6j9D"
      },
      "id": "S1WiMX3O6j9D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decision Tree"
      ],
      "metadata": {
        "id": "qvT8K5hP-k3L"
      },
      "id": "qvT8K5hP-k3L"
    },
    {
      "cell_type": "code",
      "source": [
        "# BoW\n",
        "classical_model(train_df, test_df, bow = True, model = DecisionTreeClassifier())"
      ],
      "metadata": {
        "id": "vT-FItRP8e4Z"
      },
      "id": "vT-FItRP8e4Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "classical_model(train_df, test_df, TFIDF=True, model = DecisionTreeClassifier())"
      ],
      "metadata": {
        "id": "GAo8HQtb-5Pd"
      },
      "id": "GAo8HQtb-5Pd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N-gram\n",
        "classical_model(train_df, test_df, Ngram=(1,2), model = DecisionTreeClassifier())"
      ],
      "metadata": {
        "id": "-vji7XOA_AO0"
      },
      "id": "-vji7XOA_AO0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Forest"
      ],
      "metadata": {
        "id": "dwaWLGzm-oRK"
      },
      "id": "dwaWLGzm-oRK"
    },
    {
      "cell_type": "code",
      "source": [
        "# BoW\n",
        "classical_model(train_df, test_df, bow = True, model = RandomForestClassifier())"
      ],
      "metadata": {
        "id": "tptB-PMQ843M"
      },
      "id": "tptB-PMQ843M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "classical_model(train_df, test_df, TFIDF=True, model = RandomForestClassifier())"
      ],
      "metadata": {
        "id": "LAt6ypJw_IMc"
      },
      "id": "LAt6ypJw_IMc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N-gram\n",
        "classical_model(train_df, test_df, Ngram=(1,2), model = RandomForestClassifier())"
      ],
      "metadata": {
        "id": "fMF3XoL9_MjE"
      },
      "id": "fMF3XoL9_MjE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVM"
      ],
      "metadata": {
        "id": "HOEIjGhjife2"
      },
      "id": "HOEIjGhjife2"
    },
    {
      "cell_type": "code",
      "source": [
        "# BoW\n",
        "classical_model(train_df, test_df, bow = True, model = SVC(kernel='linear'))"
      ],
      "metadata": {
        "id": "0CrmJ8sWig78"
      },
      "id": "0CrmJ8sWig78",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "classical_model(train_df, test_df, TFIDF=True, model = SVC(kernel='linear'))"
      ],
      "metadata": {
        "id": "R8dYheYritmr"
      },
      "id": "R8dYheYritmr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N-gram\n",
        "classical_model(train_df, test_df, Ngram=(1,2), model = SVC(kernel='linear'))"
      ],
      "metadata": {
        "id": "CNS6vtExivcx"
      },
      "id": "CNS6vtExivcx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Deep Learning models with Doc2Vec method"
      ],
      "metadata": {
        "id": "JlB698sc1QAb"
      },
      "id": "JlB698sc1QAb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Without pretrain model"
      ],
      "metadata": {
        "id": "fTIZEqcS2SgK"
      },
      "id": "fTIZEqcS2SgK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Doc2Vec with Neural Network"
      ],
      "metadata": {
        "id": "V4xm6-2Z-P3u"
      },
      "id": "V4xm6-2Z-P3u"
    },
    {
      "cell_type": "code",
      "source": [
        "# TƒÉng k√≠ch th∆∞·ªõc vector v√† s·ªë epoch cho Doc2Vec\n",
        "def train_doc2vec(tagged_data, vector_size=300, window=5, epochs=50):  # TƒÉng vector_size v√† epochs\n",
        "    \"\"\"\n",
        "    Train a Doc2Vec model.\n",
        "    \"\"\"\n",
        "    model_d2v = Doc2Vec(vector_size=vector_size, window=window, min_count=2, workers=4, epochs=epochs)\n",
        "    model_d2v.build_vocab(tagged_data)\n",
        "    model_d2v.train(tagged_data, total_examples=model_d2v.corpus_count, epochs=model_d2v.epochs)\n",
        "    return model_d2v\n",
        "\n",
        "# Hu·∫•n luy·ªán l·∫°i m√¥ h√¨nh Doc2Vec v·ªõi c√°c tham s·ªë m·ªõi\n",
        "model_d2v = train_doc2vec(tagged_data)\n",
        "\n",
        "# L·∫•y vector cho c√°c t·∫≠p train, validation v√† test\n",
        "train_vectors = get_vectors(model_d2v, train_df['text_clean'])\n",
        "valid_vectors = get_vectors(model_d2v, valid_df['text_clean'])\n",
        "test_vectors = get_vectors(model_d2v, test_df['text_clean'])"
      ],
      "metadata": {
        "id": "GP7_v228hQZz"
      },
      "id": "GP7_v228hQZz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "def build_nn_model(input_dim, num_labels):\n",
        "    \"\"\"\n",
        "    Build a more complex neural network classifier with Dropout for regularization.\n",
        "    \"\"\"\n",
        "    model_nn = Sequential()\n",
        "    model_nn.add(Dense(512, activation='relu', input_shape=(input_dim,)))  # TƒÉng s·ªë l∆∞·ª£ng neuron\n",
        "    model_nn.add(Dropout(0.5))  # Th√™m Dropout ƒë·ªÉ tr√°nh overfitting\n",
        "    model_nn.add(Dense(256, activation='relu'))  # Th√™m m·ªôt l·ªõp n·ªØa\n",
        "    model_nn.add(Dropout(0.5))  # Dropout ·ªü l·ªõp th·ª© hai\n",
        "    model_nn.add(Dense(num_labels, activation='softmax'))  # L·ªõp output\n",
        "    model_nn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model_nn\n"
      ],
      "metadata": {
        "id": "i7IDFaYeheVH"
      },
      "id": "i7IDFaYeheVH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X√¢y d·ª±ng m√¥ h√¨nh NN\n",
        "model_nn = build_nn_model(input_dim=300, num_labels=num_labels)\n",
        "\n",
        "# Th√™m EarlyStopping v√† ReduceLROnPlateau\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=3, min_lr=1e-6)\n",
        "\n",
        "# Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi Callbacks\n",
        "history = model_nn.fit(\n",
        "    train_vectors, train_df['Emotion'].values,\n",
        "    epochs=25, batch_size=64,\n",
        "    validation_data=(valid_vectors, valid_df['Emotion'].values),\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "# ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test\n",
        "def evaluate_nn_model(model_nn, test_vectors, test_labels):\n",
        "    \"\"\"\n",
        "    Evaluate the neural network on the test set.\n",
        "    \"\"\"\n",
        "    preds = np.argmax(model_nn.predict(test_vectors), axis=1)\n",
        "    print(\"Doc2Vec + Neural Network Classification Report:\")\n",
        "    print(classification_report(test_labels, preds))\n",
        "\n",
        "# ƒê√°nh gi√° m√¥ h√¨nh\n",
        "evaluate_nn_model(model_nn, test_vectors, test_df['Emotion'].values)"
      ],
      "metadata": {
        "id": "A8An7kkZiAkH"
      },
      "id": "A8An7kkZiAkH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bert"
      ],
      "metadata": {
        "id": "goww58xZ3iWA"
      },
      "id": "goww58xZ3iWA"
    },
    {
      "cell_type": "code",
      "source": [
        "df = Dataset.from_pandas(df).remove_columns([\"text_len\", \"Sentence\", \"__index_level_0__\"])"
      ],
      "metadata": {
        "id": "fhQi7-Kq3tJc"
      },
      "id": "fhQi7-Kq3tJc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First split: train + validation and test\n",
        "train_valid_df = df.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "# Second split: train and validation from the training portion\n",
        "train_df = train_valid_df[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "# Accessing the final train, validation, and test sets\n",
        "train_dataset = train_df[\"train\"]\n",
        "valid_dataset = train_df[\"test\"]\n",
        "test_dataset = train_valid_df[\"test\"]"
      ],
      "metadata": {
        "id": "TOLjeCPn3u3g"
      },
      "id": "TOLjeCPn3u3g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BERT tokenizer and model\n",
        "bert_check_point = \"google-bert/bert-base-multilingual-uncased\"\n",
        "label2id = {'Disgust': 0, 'Enjoyment': 1 , 'Anger': 2, 'Surprise': 3, 'Sadness': 4, 'Fear': 5, 'Other': 6}\n",
        "id2label = {0: 'Disgust',1: 'Enjoyment',2: 'Anger',3: 'Surprise',4: 'Sadness',5: 'Fear',6: 'Other'}\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(bert_check_point)\n",
        "bert_model = AutoModelForSequenceClassification.from_pretrained(bert_check_point,\n",
        "                                                                num_labels=num_labels,\n",
        "                                                                id2label=id2label,\n",
        "                                                                label2id=label2id)\n",
        "bert_model.to(device)"
      ],
      "metadata": {
        "id": "XXMJiqnF3v_a"
      },
      "id": "XXMJiqnF3v_a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(dataset, tokenizer):\n",
        "    def tokenize_function(examples):\n",
        "        # Tokenize inputs and targets\n",
        "        model_inputs = tokenizer(\n",
        "            examples[\"text_clean\"],\n",
        "            truncation=True,\n",
        "        )\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "    # Apply tokenization to dataset\n",
        "    tokenized_data = dataset.map(tokenize_function, batched=True)\n",
        "    return tokenized_data"
      ],
      "metadata": {
        "id": "9BKJbHSh3x6x"
      },
      "id": "9BKJbHSh3x6x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_df = preprocess_data(train_dataset, bert_tokenizer)\n",
        "tokenized_valid_df = preprocess_data(valid_dataset, bert_tokenizer)\n",
        "tokenized_test_df = preprocess_data(test_dataset, bert_tokenizer)"
      ],
      "metadata": {
        "id": "zAJZAPt131As"
      },
      "id": "zAJZAPt131As",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = bert_check_point.split(\"/\")[-1]\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"{model_name}-finetuned-VSMEC\",\n",
        "    evaluation_strategy=\"steps\",  # Evaluate every few steps\n",
        "    eval_steps=200,               # Adjust based on your dataset size and training speed\n",
        "    logging_steps=200,            # Log metrics more frequently\n",
        "    save_steps=200,               # Ensure saving steps is a round multiple of eval_steps\n",
        "    load_best_model_at_end=True,  # Automatically load the best model found during training\n",
        "    metric_for_best_model=\"accuracy\", # Choose the metric you consider most important\n",
        "    greater_is_better=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "vk8GJaLo31b1"
      },
      "id": "vk8GJaLo31b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\"accuracy\": accuracy_score(labels, predictions)}"
      ],
      "metadata": {
        "id": "5njAMy7932dh"
      },
      "id": "5njAMy7932dh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=bert_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_df,\n",
        "    eval_dataset=tokenized_valid_df,\n",
        "    tokenizer=bert_tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "yS8VhG1X33lU"
      },
      "id": "yS8VhG1X33lU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load PhoBERT tokenizer and model\n",
        "phobert_check_point = \"vinai/phobert-base\"\n",
        "phobert_tokenizer = AutoTokenizer.from_pretrained(phobert_check_point)\n",
        "phobert_model = AutoModelForSequenceClassification.from_pretrained(phobert_check_point, num_labels=num_labels)\n",
        "phobert_model.to(device)"
      ],
      "metadata": {
        "id": "Veb1V_Km36q5"
      },
      "id": "Veb1V_Km36q5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_df = preprocess_data(train_dataset, phobert_tokenizer)\n",
        "tokenized_valid_df = preprocess_data(valid_dataset, phobert_tokenizer)\n",
        "tokenized_test_df = preprocess_data(test_dataset, phobert_tokenizer)"
      ],
      "metadata": {
        "id": "hNAYzaGj38HL"
      },
      "id": "hNAYzaGj38HL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = phobert_check_point.split(\"/\")[-1]\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=phobert_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_df,\n",
        "    eval_dataset=tokenized_valid_df,\n",
        "    tokenizer=bert_tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "8iI_SW2i39Jz"
      },
      "id": "8iI_SW2i39Jz",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}